{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3yoyuAnnwwP4"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EZeLgicvwwP6"
   },
   "source": [
    "# Module 2 Final Project Specifications: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gklyzeSFwwP9"
   },
   "source": [
    "In this lesson, we'll review all the guidelines and specifications for the final project for Module 2. \n",
    "\n",
    "### Objectives\n",
    "\n",
    "* Understand all required aspects of the Final Project for Module 2\n",
    "* Understand all required deliverables\n",
    "* Understand what constitutes a successful project\n",
    "\n",
    "### Final Project Summary\n",
    "\n",
    "Another module down--you're half way there!\n",
    "\n",
    "< img src='halfway-there.gif'>\n",
    "\n",
    "For the culmination of Module 2, you just need to complete the final project!\n",
    "\n",
    "### The Project\n",
    "\n",
    "For this project, you'll be working with the Northwind database--a free, open-source dataset created by Microsoft containing data from a fictional company. You probably remember the Northwind database from our section on Advanced SQL. Here's the schema for the Northwind database:\n",
    "\n",
    "<img src='https://github.com/jirvingphd/dsc-2-final-project-online-ds-ft-021119/blob/master/Northwind_ERD.png?raw=1' width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pgOCAkAHwwP-"
   },
   "source": [
    "## The Deliverables\n",
    "> **The goal of this project** is to test your ability to gather information from a real-world database and use your knowledge of statistical analysis and hypothesis testing to generate analytical insights that can be of value to the company.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A4eo2x98wwQA"
   },
   "source": [
    "In addition to answering this question with a hypothesis test, you will also need to come up with **_at least 3 other hypotheses to test on your own_**.  \n",
    "\n",
    "These can by **anything that you think could be imporant information _for the company_.** \n",
    "\n",
    "For this hypothesis, be sure to specify both the **_null hypothesis_** and the **_alternative hypothesis_** for your question.  You should also specify if this is one-tail or a two-tail test. \n",
    "\n",
    "To complete this project, you will need to turn in the following 3 deliverables:\n",
    "\n",
    "1. A **_Jupyter Notebook_** containing any code you've written for this project. \n",
    "2. A **_Blog Post_** explaining your process, methodology, and findings.  \n",
    "3. An **_\"Executive Summary\" PowerPoint Presentation_** that explains the hypothesis tests you ran, your findings, and their relevance to company stakeholders.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-F7VwA6BwwQB"
   },
   "source": [
    "### Jupyter Notebook Must-Haves\n",
    "\n",
    "For this project, your jupyter notebook should meet the following specifications:\n",
    "\n",
    "**_Organization/Code Cleanliness_**\n",
    "\n",
    "* The notebook should be well organized, easy to follow, and code is commented where appropriate.  \n",
    "<br>  \n",
    "    * Level Up: The notebook contains well-formatted, professional looking markdown cells explaining any substantial code. All functions have docstrings that act as professional-quality documentation.  \n",
    "<br>      \n",
    "* The notebook is written to technical audiences with a way to both understand your approach and reproduce your results. The target audience for this deliverable is other data scientists looking to validate your findings.  \n",
    "<br>    \n",
    "* Any SQL code written to source data should also be included.  \n",
    "\n",
    "**_Findings_**\n",
    "\n",
    "* Your notebook should clearly show how you arrived at your results for each hypothesis test, including how you calculated your p-values.   \n",
    "<br>\n",
    "* You should also include any other statistics that you find relevant to your analysis, such as effect size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PXe-ZIk3wwQC"
   },
   "source": [
    "### Executive Summary Must-Haves\n",
    "\n",
    "Your presentation should:\n",
    "\n",
    "* Contain between 5-10 professional quality slides detailing:\n",
    "<br>  \n",
    "    * A high-level overview of your methodology  \n",
    "    <br>  \n",
    "    * The results of your hypothesis tests  \n",
    "    <br>  \n",
    "    * Any real-world recommendations you would like to make based on your findings (ask yourself--why should the executive team care about what you found? How can your findings help the company?)  \n",
    "    <br>  \n",
    "* Take no more than 5 minutes to present  \n",
    "<br>  \n",
    "* Avoid technical jargon and explain results in a clear, actionable way for non-technical audiences.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z6jWCSDawwQD"
   },
   "source": [
    "### Blog Post Must-Haves\n",
    "\n",
    "Your blog post should include everything from how you identified what tables contained the information you need, to how you retrieved it using SQL (and any challenges you ran into while doing so), as well as your methodology and results for your hypothesis tests. \n",
    "\n",
    "**_NOTE:_**  This blog post is your way of showcasing the work you've done on this project--chances are it will soon be read by a recruiter or hiring manager! Take the time to make sure that you craft your story well, and clearly explain your process and findings in a way that clearly shows both your technical expertise **_and_** your ability to communicate your results!\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RXO-WQHYwwQF"
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hLrUyfOEwwQG"
   },
   "source": [
    "# Outline of Data Processing and Analysis<br> (using OSEMN model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q70itr8BwwQH"
   },
   "source": [
    "1. **OBTAIN:**\n",
    "    - **Import data, inspect, check for datatypes to convert and null values**<br>\n",
    "        - Display header and info\n",
    "        - Drop any unneeded columns (df.drop(['col1','col2'],axis=1)\n",
    "\n",
    "2. **SCRUB: cast data types, identify outliers, check for multicollinearity, normalize data**<br>\n",
    "    - Check and cast data types\n",
    "        - [ ] Check for #'s that are store as objects (df.info())\n",
    "            - when converting to #'s, look for odd values (like many 0's), or strings that can't be converted\n",
    "            - Decide how to deal weird/null values (df.unique(), df.isna().sum(), df.describe()-min/max, etc\n",
    "        - [ ]  Check for categorical variables stored as integers\n",
    "    - [ ] Check for missing values  (df.isna().sum())\n",
    "        - Can drop rows or colums\n",
    "        - For missing numeric data with median or bin/convert to categorical\n",
    "        - For missing categorical data: make NaN own category OR replace with most common category\n",
    "    - [ ] Check for multicollinearity\n",
    "         - use seaborn to make correlation matrix plot [Evernote Link](https://www.evernote.com/l/AArNyaEwjA5JUL6I9PazHs_ts_hU-m7ja1I/) \n",
    "        - Good rule of thumb is anything over 0.75 corr is high, remove the variable that has the most correl with the largest # of variables\n",
    "    - [ ] Normalize data (may want to do after some exploring)\n",
    "        - Most popular is Z-scoring (but won't fix skew) \n",
    "        - Can log-transform to fix skewed data\n",
    "    \n",
    "            \n",
    "3. **EXPLORE:Check distributions, outliers, etc**\n",
    "    - [ ] Check scales, ranges (df.describe())\n",
    "    - [ ] Check histograms to get an idea of distributions (df.hist()) and dat transformations to perform\n",
    "        - Can also do kernel density estimates\n",
    "    - [ ] Use scatterplots to check for linearity and possible categorical variables (df.plot(kind-'scatter')\n",
    "        - categoricals will look like vertical lines\n",
    "    - [ ] Use pd.plotting.scatter_matrix to visualize possible relationships\n",
    "    - [ ] Check for linearity\n",
    "\n",
    "   \n",
    "4. **FIT AN INITIAL MODEL:** \n",
    "    - Various forms, detail later...\n",
    "    - **Assessing the model:**\n",
    "        - Assess parameters (slope,intercept)\n",
    "        - Check if the model explains the variation in the data (RMSE, F, R_square)\n",
    "        - *Are the coeffs, slopes, intercepts in appropriate units?*\n",
    "        - *Whats the impact of collinearity? Can we ignore?*\n",
    "5. **Revise the fitted model**\n",
    "    - Multicollinearity is big issue for lin regression and cannot fully remove it\n",
    "    - Use the predictive ability of model to test it (like R2 and RMSE)\n",
    "    - Check for missed non-linearity\n",
    "6. **Holdout validation / Train/test split**\n",
    "    - use sklearn train_test_split \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7i8Ev_kKwwQJ"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ubtNFAFewwQK"
   },
   "source": [
    "# EXPERIMENTAL DESIGN PLANNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7rzJx4u8wwQL"
   },
   "source": [
    "## Hypothesis Testing\n",
    "\n",
    "You will need query the database to get the data needed to perform a statistical analysis.  In this statistical analysis, **you'll need to perform a hypothesis test (or perhaps several) to answer the following question:**\n",
    "\n",
    "\n",
    "> **_Do discounts have a statistically significant effect on the number of products customers order? If so, at what level(s) of discount?_**\n",
    "\n",
    "### Hypothesis 1: (Given Hypothesis)\n",
    "\n",
    "- $H_1$: The greater the discount, the greater the quantity of products ordered by individual customers?\n",
    "- $H_0$:  Discounts have no effect on the quantity of product ordered by individual customers.  \n",
    "<br>\n",
    "- **Specific Aims:**\n",
    "\n",
    "    - **Aim 1:To select dataset that allows us to compare our target variable, **quantity _per purchase_**, vs the predictor variable discount.**\n",
    "    \n",
    "       - [ ] SELECT OrderID, QUantity,Discount FROM OrderDetails\n",
    "        - GROUPBY discount\n",
    "        - id = OrderId\\\n",
    "        \n",
    "    - **Aim 2: To test the relationship using a[ ]-tailed t-test.**\n",
    "        - One or two tailed?\n",
    "            - __ tailed, because...\n",
    "    - **Aim 3: To determine which level of discounts affect quantity?**\n",
    "    \n",
    " \n",
    "<br>\n",
    "\n",
    "### Hypothesis 2:\n",
    "\n",
    "- $H_1$: Customers buy more of other store items when they are buying discounted items.\n",
    "- $H_0$: Customers buy the same ***amount*** of items\n",
    "    - **amount could mean:**\n",
    "        - the # of items (discounted vs not discounted vs same level of discount?)\n",
    "        - total price per order with/without discounted products\n",
    "<br>\n",
    "- **Specific Aims:**\n",
    "\n",
    "    - Aim 1:To select dataset that allows us to compare our target variable [    ] vs the predictor variable [      ].\n",
    "    - Aim 2: To test the relationship using a [ ] -tailed t-test. \n",
    "    - Aim 3: To determine which levels of the predictor underly the effect.\n",
    "\n",
    "### Hypothesis 3:\n",
    "\n",
    "- $H_1$: \n",
    "- $H_0$:   \n",
    "<br>\n",
    "- **Specific Aims:**\n",
    "\n",
    "    - Aim 1:To select dataset that allows us to compare our target variable [    ] vs the predictor variable [      ].\n",
    "    - Aim 2: To test the relationship using a [ ] -tailed t-test. \n",
    "    - Aim 3: To determine which levels of the predictor underly the effect.\n",
    "    \n",
    "### Hypothesis 4:\n",
    "- $H_1$: \n",
    "- $H_0$:   \n",
    "<br>\n",
    "- **Specific Aims:**\n",
    "\n",
    "    - Aim 1:To select dataset that allows us to compare our target variable [    ] vs the predictor variable [      ].\n",
    "    - Aim 2: To test the relationship using a [ ] -tailed t-test. \n",
    "    - Aim 3: To determine which levels of the predictor underly the effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Oz3uzog5wwQM"
   },
   "source": [
    "# INITIALIZATION AND DECLARATION OF FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PVjqeNPowwQN"
   },
   "source": [
    "## Import packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vl-HKeJJwwQP"
   },
   "outputs": [],
   "source": [
    "# Normal packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "%matplotlib inline\n",
    "\n",
    "# Statsmodels\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.stats.api as sms\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "\n",
    "# Counter\n",
    "from collections import Counter\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import Session, sessionmaker\n",
    "from sqlalchemy import inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s6AsUbfMwwQU"
   },
   "source": [
    "## Define Functions (from proj 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O0xNoo6vwwQV"
   },
   "source": [
    "### def check_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UEz2Osm_wwQW"
   },
   "outputs": [],
   "source": [
    "# Check columns returns the datatype, null values and unique values of input series \n",
    "def check_column(series,nlargest='all'):\n",
    "    print(f\"Column: df['{series.name}']':\")\n",
    "    print(f\"dtype: {series.dtype}\")\n",
    "    print(f\"isna: {series.isna().sum()} out of {len(series)} - {round(series.isna().sum()/len(series)*100,3)}%\")\n",
    "        \n",
    "    print(f'\\nUnique non-na values:') #,df['waterfront'].unique())\n",
    "    if nlargest =='all':\n",
    "        print(series.value_counts())\n",
    "    else:\n",
    "        print(series.value_counts().nlargest(nlargest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GtI-okJkwwQa"
   },
   "source": [
    "### def multiplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i7lFBscHwwQb"
   },
   "outputs": [],
   "source": [
    "# MULTIPLOT\n",
    "from string import ascii_letters\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def multiplot(df):\n",
    "\n",
    "    sns.set(style=\"white\")\n",
    "\n",
    "    # Compute the correlation matrix\n",
    "    corr = df.corr()\n",
    "\n",
    "    # Generate a mask for the upper triangle\n",
    "    mask = np.zeros_like(corr, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "    # Set up the matplotlib figure\n",
    "    f, ax = plt.subplots(figsize=(16, 16))\n",
    "\n",
    "    # Generate a custom diverging colormap\n",
    "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    sns.heatmap(corr, mask=mask, annot=True, cmap=cmap, center=0,\n",
    "                \n",
    "    square=True, linewidths=.5, cbar_kws={\"shrink\": .5}) #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qk9N1jMywwQf"
   },
   "source": [
    "### def detect_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LzpDuDd5wwQg"
   },
   "outputs": [],
   "source": [
    "# Tukey's method using IQR to eliminate \n",
    "def detect_outliers(df,n,features):\n",
    "    outlier_indices = []\n",
    "    # iterate over features(columns)\n",
    "    for col in features:\n",
    "        # 1st quartile (25%)\n",
    "        Q1 = np.percentile(df[col], 25)\n",
    "        # 3rd quartile (75%)\n",
    "        Q3 = np.percentile(df[col],75)\n",
    "        # Interquartile range (IQR)\n",
    "        IQR = Q3 - Q1\n",
    "        # outlier step\n",
    "        outlier_step = 1.5 * IQR\n",
    "        # Determine a list of indices of outliers for feature col\n",
    "        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n",
    "        # append the found outlier indices for col to the list of outlier indices \n",
    "        outlier_indices.extend(outlier_list_col)\n",
    "        # select observations containing more than 2 outliers\n",
    "        outlier_indices = Counter(outlier_indices)        \n",
    "        multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n",
    "        return multiple_outliers \n",
    "# Outliers_to_drop = detect_outliers(data,2,[\"col1\",\"col2\"])\n",
    "# df.loc[Outliers_to_drop] # Show the outliers rows\n",
    "# Drop outliers\n",
    "# data= data.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GDzeDK5gwwQk"
   },
   "source": [
    "### def plot_hist_scat_sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MITbzaCcwwQl"
   },
   "outputs": [],
   "source": [
    "#SEABORN\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plots histogram and scatter (vs price) side by side\n",
    "def plot_hist_scat_sns(df,target='price'):\n",
    "    plt.style.use('dark_background')\n",
    "\n",
    "    figsize=(10,6)\n",
    "    \n",
    "    ## ----------- DEFINE AESTHETIC CUSTOMIZATIONS ----------- ##\n",
    "    # Axis Label fonts\n",
    "    fontTitle = {'fontsize': 12,\n",
    "               'fontweight': 'bold',\n",
    "                'fontfamily':'serif'}\n",
    "\n",
    "    fontAxis = {'fontsize': 10,\n",
    "               'fontweight': 'bold',\n",
    "                'fontfamily':'serif'}\n",
    "\n",
    "    fontTicks = {'fontsize': 8,\n",
    "               'fontweight':'bold',\n",
    "                'fontfamily':'serif'}\n",
    "\n",
    "    # Formatting dollar sign labels\n",
    "    fmtPrice = '${x:,.0f}'\n",
    "    tickPrice = mtick.StrMethodFormatter(fmtPrice)\n",
    "    \n",
    "\n",
    "    ## ----------- PLOTTING ----------- ##\n",
    "    \n",
    "    ## Loop through dataframe to plot\n",
    "    for column in df.describe():\n",
    "    \n",
    "        # Create figure with subplots for current column\n",
    "        # Note: in order to use identical syntax for large # of subplots (ax[i,j]), \n",
    "        #  declare an extra row of subplots to be removed later\n",
    "        fig, ax = plt.subplots(figsize=figsize, ncols=2, nrows=2)\n",
    "\n",
    "        \n",
    "        \n",
    "        ## ----- SUBPLOT 1 -----##\n",
    "        i,j = 0,0\n",
    "        ax[i,j].set_title(column.capitalize(),fontdict=fontTitle)\n",
    "        \n",
    "        # Define graphing keyword dictionaries for distplot (Subplot 1)\n",
    "        hist_kws = {\"linewidth\": 1, \"alpha\": 1, \"color\": 'blue','edgecolor':'w'}\n",
    "        kde_kws = {\"color\": \"white\", \"linewidth\": 1, \"label\": \"KDE\"}\n",
    "        \n",
    "        # Plot distplot on ax[i,j] using hist_kws and kde_kws\n",
    "        sns.distplot(df[column], norm_hist=True, kde=True,\n",
    "                     hist_kws = hist_kws, kde_kws = kde_kws,\n",
    "                     label=column+' histogram', ax=ax[i,j])\n",
    " \n",
    "\n",
    "        # Set x axis label\n",
    "        ax[i,j].set_xlabel(column.title(),fontdict=fontAxis)\n",
    "    \n",
    "        # Get x-ticks, rotate labels, and return\n",
    "        xticklab1 = ax[i,j].get_xticklabels(which = 'both')\n",
    "        ax[i,j].set_xticklabels(labels=xticklab1, fontdict=fontTicks, rotation=45)\n",
    "        ax[i,j].xaxis.set_major_formatter(mtick.ScalarFormatter())\n",
    "\n",
    "        \n",
    "        # Set y-label \n",
    "        ax[i,j].set_ylabel('Density',fontdict=fontAxis)\n",
    "        yticklab1=ax[i,j].get_yticklabels(which='both')\n",
    "        ax[i,j].set_yticklabels(labels=yticklab1,fontdict=fontTicks)\n",
    "        ax[i,j].yaxis.set_major_formatter(mtick.ScalarFormatter())\n",
    "        \n",
    "        \n",
    "        # Set y-grid\n",
    "        ax[i, j].set_axisbelow(True)\n",
    "        ax[i, j].grid(axis='y',ls='--')\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        ## ----- SUBPLOT 2-----  ##\n",
    "        i,j = 0,1\n",
    "        ax[i,j].set_title(column.capitalize(),fontdict=fontTitle)\n",
    "\n",
    "        # Define the ketword dictionaries for  scatter plot and regression line (subplot 2)\n",
    "        line_kws={\"color\":\"white\",\"alpha\":0.5,\"lw\":4,\"ls\":\":\"}\n",
    "        scatter_kws={'s': 2, 'alpha': 0.5,'marker':'.','color':'blue'}\n",
    "\n",
    "        # Plot regplot on ax[i,j] using line_kws and scatter_kws\n",
    "        sns.regplot(df[column], df[target], \n",
    "                    line_kws = line_kws,\n",
    "                    scatter_kws = scatter_kws,\n",
    "                    ax=ax[i,j])\n",
    "        \n",
    "        # Set x-axis label\n",
    "        ax[i,j].set_xlabel(column.title(),fontdict=fontAxis)\n",
    "\n",
    "         # Get x ticks, rotate labels, and return\n",
    "        xticklab2=ax[i,j].get_xticklabels(which='both')\n",
    "        ax[i,j].set_xticklabels(labels=xticklab2,fontdict=fontTicks, rotation=45)\n",
    "        ax[i,j].xaxis.set_major_formatter(mtick.ScalarFormatter())\n",
    "\n",
    "        # Set  y-axis label\n",
    "        ax[i,j].set_ylabel(target,fontdict=fontAxis)\n",
    "        \n",
    "        # Get, set, and format y-axis Price labels\n",
    "        yticklab = ax[i,j].get_yticklabels()\n",
    "        ax[i,j].set_yticklabels(yticklab,fontdict=fontTicks)\n",
    "        ax[i,j].yaxis.set_major_formatter(mtick.ScalarFormatter())\n",
    "\n",
    "#         ax[i,j].get_yaxis().set_major_formatter(tickPrice) \n",
    "\n",
    "        # Set y-grid\n",
    "        ax[i, j].set_axisbelow(True)\n",
    "        ax[i, j].grid(axis='y',ls='--')       \n",
    "        \n",
    "        ## ---------- Final layout adjustments ----------- ##\n",
    "        # Deleted unused subplots \n",
    "        fig.delaxes(ax[1,1])\n",
    "        fig.delaxes(ax[1,0])\n",
    "\n",
    "        # Optimizing spatial layout\n",
    "        fig.tight_layout()\n",
    "        figtitle=column+'_dist_regr_plots.png'\n",
    "        plt.savefig(figtitle)\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BVwy49ADwwQn"
   },
   "source": [
    "## Defining Functions (Proj 2 specific)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nkPB_HcdwwQo"
   },
   "source": [
    "### def list2df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TwdV-j85wwQp"
   },
   "outputs": [],
   "source": [
    "def list2df(list):#, sort_values='index'):\n",
    "    \"\"\" Take in a list where row[0] = column_names and outputs a dataframe.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    set_index -- df.set_index(set_index)\n",
    "    sortby -- df.sorted()\n",
    "    \"\"\"    \n",
    "    \n",
    "    df_list = pd.DataFrame(list[1:],columns=list[0])\n",
    "#     df_list = df_list[1:]\n",
    "\n",
    "    return df_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ekTOehxHwwQu"
   },
   "source": [
    "### def get_col_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LOaNcafFwwQv"
   },
   "outputs": [],
   "source": [
    "def  get_col_info(col_name):\n",
    "    \n",
    "    col_list = inspector.get_columns(col_name)\n",
    "    \n",
    "    column_info = [['table','column','dtype']]\n",
    "    print(f'Table Name: {col_name}\\n')\n",
    "\n",
    "    for col in col_list:\n",
    "        column_info.append([str(col_name),col['name'], col['type']])\n",
    "        \n",
    "    df = list2df(column_info)\n",
    "    return column_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XrvSuzkKwwQz"
   },
   "source": [
    "### def get_full_table_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A0bruXiLwwQ0"
   },
   "outputs": [],
   "source": [
    "def  get_full_table_info(engine):\n",
    "    \n",
    "    column_info = [['table','column','dtype']]\n",
    "    \n",
    "    list_tables= engine.table_names()\n",
    "    \n",
    "    for table in list_tables:\n",
    "        \n",
    "        col_list = inspector.get_columns(table)\n",
    "        \n",
    "        for col in col_list:\n",
    "            \n",
    "            column_info.append([str(table),col['name'], col['type'],col['']])\n",
    "            inspector.get_foreign_keys()\n",
    "    \n",
    "    df = list2df(column_info)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ne6vEM4CwwQ3"
   },
   "outputs": [],
   "source": [
    "# From Mike\n",
    "def describe_outliers(df):\n",
    "    out_count = 0\n",
    "    new_df = pd.DataFrame(columns=['total_outliers', 'percent_total'])\n",
    "    for col in df.columns:\n",
    "        outies = find_outliers(df[col])\n",
    "        out_count += len(outies) \n",
    "        new_df.loc[col] = [len(outies), round((len(outies)/len(df.index))*100, 2)]\n",
    "    new_df.loc['grand_total'] = [sum(new_df['total_outliers']), sum(new_df['percent_total'])]\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7mKcl3fkwwQ6"
   },
   "source": [
    "# IMPORTING DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jNCeTHeV4W9t"
   },
   "source": [
    "## **Hey Mike,**\n",
    "We must change up the start of the script in order to get the sqlite file loaded properly.\n",
    "We basically mount google drive and then tell it the filepath.<br> \n",
    "If you open the sidebar on the left, there is  a Files tab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "N0O4-cWq1N53",
    "outputId": "eeab192a-9a4b-4bf2-a7f0-93e8258bd395"
   },
   "outputs": [],
   "source": [
    "# #The northwind.sqlite is located in: content/drive/My Drive/Colab Notebooks/datasets/Northwind_small.sqlite\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive') \n",
    "\n",
    "#!ln -s \"gdrive/My Drive/Colab Notebooks/\" # \"/content/Colab Notebooks\" # I tried to make a link to just load folder but it didn't work. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3761
    },
    "colab_type": "code",
    "id": "NGfWlIw45Xs1",
    "outputId": "47f890d0-a69e-461d-fb1c-03c5cee8f9fc"
   },
   "outputs": [],
   "source": [
    "\n",
    "# filepath = '/content/drive/My Drive/Northwind_small.sqlite'\n",
    "# #-------------------\n",
    "# # Testing minimal version of prior code\n",
    "# import sqlite3\n",
    "# from sqlalchemy import create_engine, inspect\n",
    "# # from sqlalchemy import Table, Column, Integer, String, MetaData, ForeignKey,text, Float\n",
    "\n",
    "# engine = create_engine('sqlite:///content/drive/My Drive/Northwind_small.sqlite',echo=True)\n",
    "# inspector = inspect(engine);\n",
    "# db_tables = inspector.get_table_names()\n",
    "# print('\\n',db_tables);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zEqu9PbxwwQ8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-03 09:47:50,913 INFO sqlalchemy.engine.base.Engine SELECT CAST('test plain returns' AS VARCHAR(60)) AS anon_1\n",
      "2019-04-03 09:47:50,915 INFO sqlalchemy.engine.base.Engine ()\n",
      "2019-04-03 09:47:50,916 INFO sqlalchemy.engine.base.Engine SELECT CAST('test unicode returns' AS VARCHAR(60)) AS anon_1\n",
      "2019-04-03 09:47:50,918 INFO sqlalchemy.engine.base.Engine ()\n",
      "2019-04-03 09:47:50,920 INFO sqlalchemy.engine.base.Engine SELECT name FROM sqlite_master WHERE type='table' ORDER BY name\n",
      "2019-04-03 09:47:50,921 INFO sqlalchemy.engine.base.Engine ()\n",
      "\n",
      " ['Category', 'Customer', 'CustomerCustomerDemo', 'CustomerDemographic', 'Employee', 'EmployeeTerritory', 'Order', 'OrderDetail', 'Product', 'Region', 'Shipper', 'Supplier', 'Territory']\n"
     ]
    }
   ],
   "source": [
    "# Testing minimal version of prior code\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine, inspect\n",
    "from sqlalchemy import Table, Column, Integer, String, MetaData, ForeignKey,text, Float\n",
    "\n",
    "engine = create_engine(\"sqlite:///Northwind_small.sqlite\",echo=True)\n",
    "inspector = inspect(engine);\n",
    "db_tables = inspector.get_table_names()\n",
    "print('\\n',db_tables);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lz_NZXdJ55r8"
   },
   "source": [
    "### YAY! The Sqlite file is working!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1BgHrsVywwQ6"
   },
   "source": [
    "### Importing Method \n",
    "- Use sqlalchemy to create engine to connect to Northwind_small.sqlite.\n",
    "- use pd.read_sql_query('SELECT * FROM OrderDetail',egine) to directly read db into df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 117
    },
    "colab_type": "code",
    "id": "slE9zwdIwwRF",
    "outputId": "f32e8063-e850-4b27-bb2b-d465da83a4bb"
   },
   "outputs": [],
   "source": [
    "list2df(get_col_info('OrderDetail'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "grfRrQwj6Ff8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fcYXKTJe-bjt"
   },
   "source": [
    "# Hypothesis 1: discounted vs non-discounted quantites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2016
    },
    "colab_type": "code",
    "id": "8r91sfLGwwRK",
    "outputId": "921e5bcc-7521-4636-e6f5-eea5452a85bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-03 09:49:51,884 INFO sqlalchemy.engine.base.Engine SELECT * FROM OrderDetail\n",
      "2019-04-03 09:49:51,885 INFO sqlalchemy.engine.base.Engine ()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>OrderId</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UnitPrice</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10248/11</td>\n",
       "      <td>10248</td>\n",
       "      <td>11</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10248/42</td>\n",
       "      <td>10248</td>\n",
       "      <td>42</td>\n",
       "      <td>9.8</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10248/72</td>\n",
       "      <td>10248</td>\n",
       "      <td>72</td>\n",
       "      <td>34.8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10249/14</td>\n",
       "      <td>10249</td>\n",
       "      <td>14</td>\n",
       "      <td>18.6</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10249/51</td>\n",
       "      <td>10249</td>\n",
       "      <td>51</td>\n",
       "      <td>42.4</td>\n",
       "      <td>40</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Id  OrderId  ProductId  UnitPrice  Quantity  Discount\n",
       "0  10248/11    10248         11       14.0        12       0.0\n",
       "1  10248/42    10248         42        9.8        10       0.0\n",
       "2  10248/72    10248         72       34.8         5       0.0\n",
       "3  10249/14    10249         14       18.6         9       0.0\n",
       "4  10249/51    10249         51       42.4        40       0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We want to test hypothesis one, so we need OrderDetail table.\n",
    "table_to_test = \"OrderDetail\"\n",
    "df = pd.read_sql_query(\"SELECT * FROM OrderDetail\",  engine)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bHdkczoawwRO",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hOZ_8uy7HuMr"
   },
   "outputs": [],
   "source": [
    "sorted(df['Discount'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u1V-i0y6wwTB",
    "scrolled": true
   },
   "source": [
    "### Note on OrderDetails df:\n",
    "1. There are 11 possible values for discounts: \n",
    "    - Values = [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.1, 0.15, 0.2, 0.25]<br>\n",
    "    ``` print(sorted(df['Discount'].unique())) ```\n",
    "2. Too many columns to see.\n",
    "    - Can I stack/unstack to see better?\n",
    "    \n",
    "### Now how to proceed?\n",
    "1. Check what I need to run a t-test in python. \n",
    "    - [ ] Once review, use info to determine next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S7s99OFmwwRS"
   },
   "outputs": [],
   "source": [
    "df['Quantity'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hv1Y1NyQwwRW"
   },
   "outputs": [],
   "source": [
    "plt.style.use('default')\n",
    "              \n",
    "fig = plt.figure(num=1, figsize=(3,3))\n",
    "ax = fig.add_axes()\n",
    "sns.distplot(df['Quantity'],ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VX6P3oYJwwRa"
   },
   "outputs": [],
   "source": [
    "# Examining descriptive stats by discount\n",
    "df.groupby('Discount').describe().stack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xVFJ3FrcwwRe"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zx9Gwa4bwwRf"
   },
   "source": [
    "## BOOKMARK START... <img src=\"https://www.dropbox.com/s/6xqzendi1iyzls8/bookmark.png?raw=1\" width=25>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QHAKqLPM96FU"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q-uvY3EMwwRg"
   },
   "source": [
    "## Processing df into 2 for normalizaiton and ttest\n",
    "1. We were first separating the dataframe into the groups we wanted to test. \n",
    "2. **@NOW**: ADDING IN OUTLIER REMOVAL\n",
    "- separate df into 2 dfs:\n",
    "    - df_disc\n",
    "    - df_no_disc\n",
    "- run each df thought detect_outliers to get outlier index\n",
    "\n",
    "```python\n",
    "outliers_disc = detect_outliers(df_disc,0,['Quantity])\n",
    "outliers_no_disc = detect_outliers(df_no_disc,0,['Quantity])\n",
    "```\n",
    "- To see outliers just do:\n",
    "```python\n",
    "df_disc = df_disc.loc[df['Discount']>0]\n",
    "df_disc.head()\n",
    "```\n",
    "\n",
    "- To drop outliers dO:\n",
    "```python\n",
    "df_disc.loc[outliers_disc]=np.nan\n",
    "df_disc.dropna() #?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vefLX4WswwRg"
   },
   "source": [
    "#### 1. We were first separating the dataframe into the groups we wanted to test. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W_tI5InvwwRi"
   },
   "outputs": [],
   "source": [
    "df_disc = df.loc[df['Discount']>0]\n",
    "df_disc.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "16Ebxk2JwwRo"
   },
   "outputs": [],
   "source": [
    "# Examine split dataframes  - df_disc\n",
    "df_disc.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yhiC4aWY82VH"
   },
   "outputs": [],
   "source": [
    "# Separating df_no_disc\n",
    "df_no_disc = df.loc[df['Discount']==0]\n",
    "df_no_disc.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CXEftlqV9SWd"
   },
   "outputs": [],
   "source": [
    "# Examine split dataframes  - df_disc\n",
    "df_no_disc.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fq_JAeWWwwRy"
   },
   "source": [
    "#### 2. Run each df thought detect_outliers to get outlier index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eMA7n-4O9X7o"
   },
   "source": [
    "##### Run detect_outliers on df_disc and then examine stats of outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nxaepnCPwwRz"
   },
   "outputs": [],
   "source": [
    "# Detect outliers in df_disc\n",
    "outlier_disc = detect_outliers(df_disc,0,['Quantity'])\n",
    "\n",
    "print(f'There are {len(outlier_disc)} outliers out of {len(df)} = {len(outlier_disc)/len(df)*100}%')\n",
    "df[['Quantity','Discount']].iloc[outlier_disc].describe()\n",
    "# df.iloc[outlier_disc].sort_values('Quantity', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dIikkelyABAB"
   },
   "source": [
    "#### Now checking df_no_disc for outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1uQLmUcZ_5t3"
   },
   "outputs": [],
   "source": [
    "# Detect outliers in df_no_disc\n",
    "outlier_no_disc = detect_outliers(df_no_disc,0,['Quantity'])\n",
    "# df.iloc[outlier_no_disc].sort_values('Quantity', ascending=False)#.describe()\n",
    "print(f'There are {len(outlier_no_disc)} outliers out of {len(df)} = {len(outlier_no_disc)/len(df)*100}%')\n",
    "df[['Quantity','Discount']].iloc[outlier_no_disc].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kz4oI-IPwwRu"
   },
   "outputs": [],
   "source": [
    "# print(f'Checking the statstics for the outliers from df_disc...' )\n",
    "# df[['Quantity','Discount']].iloc[outlier_no_disc] #.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gGqk4D46HP4O"
   },
   "source": [
    "## Now we should test if we remove those outliers, do we pass the normality test\n",
    "\n",
    "- Hmm ok so there is somethihng a bit weird about the outlier indices. They MUST be the index from the first df before we split it.\n",
    "- This means we will have to remove outliers from full df and split again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7GUrC_QaHayz"
   },
   "outputs": [],
   "source": [
    "df_to_clean = df.copy()\n",
    "\n",
    "# I was being 'lazy' and zipping seemed easier lol\n",
    "remove_label= ['outlier_disc','outlier_no_disc']\n",
    "remove_data= [outlier_disc,outlier_no_disc]\n",
    "remove_me = dict(zip(remove_label,remove_data))\n",
    "\n",
    "# Loop and replace indices of outliers with np.nan \n",
    "for key,val in remove_me.items():\n",
    "  df_to_clean.iloc[val]=np.nan\n",
    "  print(f'{key} removed {len(val)} outliers')\n",
    "  \n",
    "  \n",
    "print('After removal:')\n",
    "print(df_to_clean.isna().sum())\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uk4M3t4gKGa9"
   },
   "outputs": [],
   "source": [
    "# Replace na and run stattest \n",
    "\n",
    "df_to_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W-5EtAuNMF84"
   },
   "outputs": [],
   "source": [
    "df_clean = df_to_clean.dropna(axis=0)\n",
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6PrPoFbsMZKb"
   },
   "source": [
    "#### NOW ADD NORMTEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I72gdFKvwwR9"
   },
   "outputs": [],
   "source": [
    "df_discounted = df_clean.loc[df_clean['Discount']>0]\n",
    "df_discounted.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aJ4PYoacM8Pn"
   },
   "outputs": [],
   "source": [
    "df_fullprice = df_clean.loc[df_clean['Discount']==0]\n",
    "df_fullprice.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B8V_fLKzNLgS"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import normaltest \n",
    "stat, p = normaltest(df_discounted['Quantity'])\n",
    "print(f'p ={p}, stat ={stat}')\n",
    "\n",
    "results_normtest = [['DataIn','stat','p']]\n",
    "results_normtest.append(['Discounted, cleaned',stat,p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L6l3S61dPEng"
   },
   "outputs": [],
   "source": [
    "# Shapiro-Wilk Test\n",
    "from numpy.random import seed\n",
    "from numpy.random import randn\n",
    "from scipy.stats import shapiro\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gnAePBXgN1BH"
   },
   "source": [
    "#### Thoughts...\n",
    "- **Hmmmm...why didn't that work? Or maybe it did and the test is too picky?**\n",
    "- What was the original data's norm test?\n",
    "  - Run again below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tciaNn_rWcAW"
   },
   "source": [
    "### DEF normtest_results to run multiple normaltests and save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hb6MwcV-POzb"
   },
   "outputs": [],
   "source": [
    "# DEFINING FUNCTION \n",
    "#data_in={'name':data}\n",
    "def normtest_results(dict_data):\n",
    "    results_normtest_shap = [['DataIn','Test','stat','p']]\n",
    "    results_normtest_dagp = [['DataIn','Test','stat','p']]\n",
    "\n",
    "    for key,val in dict_data.items():\n",
    "    \n",
    "        data_in = val\n",
    "        name = key\n",
    "        test = 'Shapiro'\n",
    "        stat, p = shapiro(data_in)\n",
    "        results_normtest_shap.append([name , test, stat , p ])\n",
    "        test = 'D’Agostino’s'\n",
    "        stat, p = normaltest(data_in)\n",
    "        results_normtest_dagp.append([name,test,stat, p])\n",
    "    \n",
    "    results_normtest = pd.concat([list2df(results_normtest_shap), list2df(results_normtest_dagp)]) \n",
    "return results_normtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QQlm4VmiUPOG"
   },
   "outputs": [],
   "source": [
    "# Building data_in dictionary for normtest_results\n",
    "data_in = {}\n",
    "\n",
    "df_disc_orig = df_disc\n",
    "df_no_disc_orig = df_no_disc\n",
    "\n",
    "data_in['Orig, Disc'] = df_disc_orig['Quantity']\n",
    "data_in['Clean,Disc'] = df_discounted['Quantity']\n",
    "data_in['Orig, Full Price'] = df_no_disc_orig['Quantity']\n",
    "data_in['Clean, Full Price'] = df_fullprice['Quantity']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j46n28zjVI1E"
   },
   "outputs": [],
   "source": [
    "results = normtest_results(data_in)\n",
    "# list2df(results)\n",
    "# results.to_csv('norm_test_stack.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "SdXLcGN2UsG8"
   },
   "outputs": [],
   "source": [
    "def norm_test_to_df(data_in):\n",
    "    results = normtest_results(data_in)\n",
    "    results_pivot = results.pivot(index='Test', columns='DataIn')#(index='DataIn', columns= 'Test') #creating pivot table \n",
    "    side_by_side = results_pivot.stack(0)\n",
    "    cols = side_by_side.columns\n",
    "    cols = [cols[-1], cols[0], cols[2], cols[1]]\n",
    "    compare_results = side_by_side[cols]\n",
    "    return compare_results\n",
    "# compare_results.pivot(index = )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_results = {results[i,'Test']:{'data':results[i,'DataIn'],\n",
    "#                                  'stat':results[i,'stat'], 'p': results[i,'p'] } for i in \n",
    "#                 results.columns()}\n",
    "# dict_results = {test:{'data':data, 'stat': stat, 'p': p} for (test, data, stat, P)\n",
    "#                  in results['Test', 'DataIn', 'stat', 'p'].iterrows()}        \n",
    "                \n",
    "#                 for test in results['Test']\n",
    "#                for data in results['DataIn'] for stat in results['stat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = results.columns[1:]\n",
    "test_names = results['Test']\n",
    "colnames, test_names.unique()\n",
    "\n",
    "new_names=[]\n",
    "for test in test_names:\n",
    "    \n",
    "    for name in colnames:\n",
    "        cat_col = [name+'-'+test]\n",
    "        new_names.append(cat_col)\n",
    "new_names\n",
    "new_results = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_discounted.info(),print('\\n'), df_fullprice.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler()\n",
    "robust_disc = scaler.fit_transform(np.array(df_discounted['Quantity']).reshape(-1,1))\n",
    "robust_full = scaler.fit_transform(np.array(df_fullprice['Quantity']).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_data_in = {}\n",
    "\n",
    "# df_disc_orig = df_disc\n",
    "# df_no_disc_orig = df_no_disc\n",
    "\n",
    "robust_data_in['robust, Disc'] = robust_disc\n",
    "robust_data_in['Clean,Disc'] = df_discounted\n",
    "robust_data_in['robust, Full Price'] = robust_full\n",
    "robust_data_in['Clean, Full Price'] = df_fullprice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_norm_test = norm_test_to_df(robust_data_in)\n",
    "robust_norm_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CQ0Sd0KjwwR7"
   },
   "source": [
    "## ...END BOOKMARK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L1-gACN4wwSC"
   },
   "source": [
    "#### Preprocc and visualizing Normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "75wHOyASwwSD"
   },
   "outputs": [],
   "source": [
    "#### Preprocc and visualizing Normalization \n",
    "\n",
    "plt.style.use('default')\n",
    "plt.style.use('seaborn')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Normalzie data with Normalzier\n",
    "from sklearn.preprocessing import Normalizer\n",
    "makeNorm = Normalizer\n",
    "\n",
    "norm_disc = makeNorm().fit_transform(np.array(disc_series).reshape(-1,1))\n",
    "norm_nondisc = makeNorm().fit_transform(np.array(nondisc_series).reshape(-1,1))\n",
    "\n",
    "plt.hist(norm_disc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9cZXxcEPwwSG"
   },
   "outputs": [],
   "source": [
    "# PLOT ORIGINAL DATA FIRST BEFORE NORMALIZING AND PLOTTING NORM DATA TO COMPARE\n",
    "fig, ax  = plt.subplots(2,2,figsize=(6,4))\n",
    "\n",
    "#subplot 1: discounted, raw\n",
    "i,j = 0,0\n",
    "sns.distplot(disc_series, ax=ax[i,j])\n",
    "ax[i,j].set_title('Raw Discounted')\n",
    "\n",
    "# subplot 2: nondiscounted, raw\n",
    "i,j=0,1\n",
    "sns.distplot(nondisc_series, ax=ax[i,j])\n",
    "ax[i,j].set_title('Raw Non-Discounted')\n",
    "\n",
    "\n",
    "#MUST NORMALIZE DATA IN THIS SECTION BEFORE CAN FINISH SUBPLOT 3 and 4\n",
    "# norm_disc = makeNorm(disc_series)\n",
    "# norm_nondisc = makeNorm(nondisc_series)\n",
    "\n",
    "\n",
    "#subplot 3: discounted,normalized\n",
    "i,j = 1,0\n",
    "sns.distplot(norm_disc, ax=ax[i,j])\n",
    "ax[i,j].set_title('Normalized Discounted')\n",
    "\n",
    "# subplot 2: nondiscounted, normalized\n",
    "i,j=1,1\n",
    "sns.distplot(norm_nondisc, ax=ax[i,j])\n",
    "ax[i,j].set_title('Normalized Non-Discounted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ifMKcruHwwSK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fgA6YLcCwwSQ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.style.use(\"dark_background\")\n",
    "#,'')\n",
    "# fig,ax = plt.figure()\n",
    "plt.hist(disc_series,bins=20,alpha=0.5,edgecolor='k',label='Discounted')\n",
    "plt.hist(nondisc_series,alpha=0.5, bins=20, edgecolor='k',label='FUll Price')\n",
    "plt.xlabel('')\n",
    "plt.ylabel('# of observations')\n",
    "plt.legend()\n",
    "# fig\n",
    "plt.axvline(np.mean(disc_series),ls=':',label='disc')\n",
    "plt.axvline(np.mean(nondisc_series),ls=':',color='purple',label='non disc')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2y_dPEkwwwSc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ddJDsYLRwwSf"
   },
   "outputs": [],
   "source": [
    "samp_disc = np.random.choice(disc_series,size=200)\n",
    "np.mean(samp_disc)\n",
    "plt.hist(samp_disc)`\n",
    "outliers = detect_outliers(disc_series,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-D6E8p6EwwSi"
   },
   "outputs": [],
   "source": [
    "stat1, p1 = normaltest(disc_series) \n",
    "stat2, p2 = normaltest(nondisc_series) \n",
    "\n",
    "p1, p2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PKDwFRjcwwSn"
   },
   "source": [
    "#### Checking the distributions of both groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LxQ61ajzwwSo"
   },
   "outputs": [],
   "source": [
    "# running 2-sample independent ttest on disc vs nondisc\n",
    "from scipy.stats import ttest_ind\n",
    "stat, p = ttest_ind(np.array(disc_series),np.array(nondisc_series))\n",
    "print(stat,p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gOQDKhGvwwSr"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vxwz701mwwSs"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ows6lYhEwwSt"
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from scipy import * \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GB6YpVyEwwSw"
   },
   "outputs": [],
   "source": [
    " np.random.seed(12345678)\n",
    "\n",
    "\n",
    "rvs1 = stats.norm.rvs(loc=5,scale=10,size=500)\n",
    "rvs2 = stats.norm.rvs(loc=5,scale=10,size=500)\n",
    "stats.ttest_ind(rvs1,rvs2)\n",
    "(0.26833823296239279, 0.78849443369564776)\n",
    "stats.ttest_ind(rvs1,rvs2, equal_var = False)\n",
    "(0.26833823296239279, 0.78849452749500748)\n",
    "ttest_ind underestimates p for unequal variances:\n",
    "\n",
    "\n",
    "rvs3 = stats.norm.rvs(loc=5, scale=20, size=500)\n",
    "stats.ttest_ind(rvs1, rvs3)\n",
    "(-0.46580283298287162, 0.64145827413436174)\n",
    "stats.ttest_ind(rvs1, rvs3, equal_var = False)\n",
    "(-0.46580283298287162, 0.64149646246569292)\n",
    "When n1 != n2, the equal variance t-statistic is no longer equal to the unequal variance t-statistic:\n",
    "\n",
    "\n",
    " rvs4 = stats.norm.rvs(loc=5, scale=20, size=100)\n",
    " stats.ttest_ind(rvs1, rvs4)\n",
    "(-0.99882539442782481, 0.3182832709103896)\n",
    " stats.ttest_ind(rvs1, rvs4, equal_var = False)\n",
    "(-0.69712570584654099, 0.48716927725402048)\n",
    "T-test with different means, variance, and n:\n",
    "\n",
    "\n",
    " rvs5 = stats.norm.rvs(loc=8, scale=20, size=100)\n",
    " stats.ttest_ind(rvs1, rvs5)\n",
    "(-1.4679669854490653, 0.14263895620529152)\n",
    " stats.ttest_ind(rvs1, rvs5, equal_var = False)\n",
    "(-0.94365973617132992, 0.34744170334794122)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AG9-mbJfwwS0"
   },
   "outputs": [],
   "source": [
    "check_column(df['Quantity'],20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NjZDQQJrwwS4"
   },
   "outputs": [],
   "source": [
    "# Examine df stats by discount level\n",
    "df.groupby('Discount').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6LZfVxkRwwS8"
   },
   "outputs": [],
   "source": [
    "print(sorted(df['Discount'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P23rLwIWwwTC"
   },
   "source": [
    "## Exploring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iYAYuIgLwwTD",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_hist_scat_sns(df,'Quantity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7hWwTGx1wwTG"
   },
   "source": [
    "### NEXT FIGURE OUT HOW TO EXAMINE THE KDE FOR ...EACH Discount level?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rx6xx9R2wwTH"
   },
   "outputs": [],
   "source": [
    "# Show counts of discounts by qunatit\n",
    "# df.apply(pd.Series.value_counts)\n",
    "test = df.iloc[df.groupby(['Discount'])].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FesF_qLZwwTJ"
   },
   "outputs": [],
   "source": [
    "df_disc_quant = df.groupby(['Discount','Quantity']).count() #.plot(kind='bar')\n",
    "df_disc_quant.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ehqav8C9wwTK"
   },
   "outputs": [],
   "source": [
    "# # Testing extracting info from groupby and describe\n",
    "# df_test = df.groupby('Discount')#.describe().stack()\n",
    "# df_cols = df_test.columns\n",
    "# df_cols\n",
    "# # # Turn df_cols into list \n",
    "# # df_cols=[x for x in df_cols]\n",
    "# # df_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5TVC2x_RwwTN"
   },
   "source": [
    "#### From following sqlalchemy tutorial slides, currently note used\n",
    "<a href=\"http://www.dropbox.com/s/rt6s9rqp06m2vky/sqlalchemy%20slides.pdf?dl=0\">Introduction to Alchemy Slides</a>\n",
    "```python\n",
    "metadata=MetaData()\n",
    "# Now use sqlalchemy\n",
    "order_table = Table('OrderDetail', metadata, \n",
    "                    Column('ProductId',Integer,primary_key=True),\n",
    "                    Column('Discount',Float),\n",
    "                    Column('OrderId',Integer),\n",
    "                    Column('Quantity',Integer))\n",
    "\n",
    "\n",
    "# order_table.create(engine)\n",
    "# metadata.create_all(engine)\n",
    "con = engine.connect()\n",
    "rs = con.execute()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VJsLFGXPwwTO",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Import sqalchemy\n",
    "# import sqlalchemy\n",
    "# from sqlalchemy import create_engine, inspect\n",
    "# from sqlalchemy.orm import Session, sessionmaker\n",
    "# from sqlalchemy import Table, Column, Integer, String, MetaData, ForeignKey,text, Float\n",
    "\n",
    "# # Create engine to connect to database\n",
    "# engine = create_engine(\"sqlite:///Northwind_small.sqlite\",echo=True)\n",
    "# Session = sessionmaker(bind=engine)\n",
    "# session = Session()\n",
    "\n",
    "# # Inspect the table names of the database. \n",
    "# inspector = inspect(engine);\n",
    "# db_tables = inspector.get_table_names()\n",
    "# print('\\n',db_tables);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-xn_WHdcwwTT"
   },
   "source": [
    "## Experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ne1PzijRwwTV"
   },
   "outputs": [],
   "source": [
    "stop\n",
    "df_tables = get_full_table_info(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "haMHgyXrwwTZ"
   },
   "outputs": [],
   "source": [
    "df_tables['table'].unique()\n",
    "df_tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZaUS7PCd08uQ"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VTVhqwhMfh13"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xqu3zJV3fq6z"
   },
   "outputs": [],
   "source": [
    "pd.read_sql_query('SELECT * FROM OrderDetail',engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a_22M7dSftkZ"
   },
   "outputs": [],
   "source": [
    "sns.jointplot(df.Discount, df.Quantity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [-1, 0, 0.05, .10, .15, .20, .25 ] # creating bins, based on what I can see in the hist.\n",
    "bins_zip = pd.cut(df['discount'], bins) # putting the data in those bins\n",
    "bins_zip = bins_zip.cat.as_unordered() #making them the correct type\n",
    "bins_zip.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyp. 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-03 09:53:48,982 INFO sqlalchemy.engine.base.Engine SELECT * FROM Order\n",
      "2019-04-03 09:53:48,983 INFO sqlalchemy.engine.base.Engine ()\n",
      "2019-04-03 09:53:48,985 INFO sqlalchemy.engine.base.Engine ROLLBACK\n"
     ]
    },
    {
     "ename": "OperationalError",
     "evalue": "(sqlite3.OperationalError) near \"Order\": syntax error [SQL: 'SELECT * FROM Order'] (Background on this error at: http://sqlalche.me/e/e3q8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.6/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_execute_context\u001b[0;34m(self, dialect, constructor, statement, parameters, *args)\u001b[0m\n\u001b[1;32m   1235\u001b[0m                     self.dialect.do_execute(\n\u001b[0;32m-> 1236\u001b[0;31m                         \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1237\u001b[0m                     )\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.6/site-packages/sqlalchemy/engine/default.py\u001b[0m in \u001b[0;36mdo_execute\u001b[0;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdo_execute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m         \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperationalError\u001b[0m: near \"Order\": syntax error",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-e497c3e8fc21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_ord\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_sql_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT * FROM Order\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_ord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.6/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mread_sql_query\u001b[0;34m(sql, con, index_col, coerce_float, params, parse_dates, chunksize)\u001b[0m\n\u001b[1;32m    312\u001b[0m     return pandas_sql.read_query(\n\u001b[1;32m    313\u001b[0m         \u001b[0msql\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoerce_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoerce_float\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         parse_dates=parse_dates, chunksize=chunksize)\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.6/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mread_query\u001b[0;34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize)\u001b[0m\n\u001b[1;32m   1061\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1064\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.6/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    952\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m         \u001b[0;34m\"\"\"Simple passthrough to SQLAlchemy connectable\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnectable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m     def read_table(self, table_name, index_col=None, coerce_float=True,\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.6/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, statement, *multiparams, **params)\u001b[0m\n\u001b[1;32m   2142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2143\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontextual_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclose_with_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2144\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmultiparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2146\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmultiparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.6/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, object_, *multiparams, **params)\u001b[0m\n\u001b[1;32m    972\u001b[0m         \"\"\"\n\u001b[1;32m    973\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 974\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    975\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobject_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute_on_connection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.6/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_execute_text\u001b[0;34m(self, statement, multiparams, params)\u001b[0m\n\u001b[1;32m   1145\u001b[0m             \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m             \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m         )\n\u001b[1;32m   1149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_events\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_events\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.6/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_execute_context\u001b[0;34m(self, dialect, constructor, statement, parameters, *args)\u001b[0m\n\u001b[1;32m   1238\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m             self._handle_dbapi_exception(\n\u001b[0;32m-> 1240\u001b[0;31m                 \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1241\u001b[0m             )\n\u001b[1;32m   1242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.6/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_handle_dbapi_exception\u001b[0;34m(self, e, statement, parameters, cursor, context)\u001b[0m\n\u001b[1;32m   1456\u001b[0m                 \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_cause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewraise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1457\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mshould_wrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                 \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_cause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlalchemy_exception\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1459\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m                 \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.6/site-packages/sqlalchemy/util/compat.py\u001b[0m in \u001b[0;36mraise_from_cause\u001b[0;34m(exception, exc_info)\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0mcause\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexc_value\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mexc_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexception\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m     \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexc_tb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcause\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.6/site-packages/sqlalchemy/util/compat.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb, cause)\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__cause__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcause\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.6/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_execute_context\u001b[0;34m(self, dialect, constructor, statement, parameters, *args)\u001b[0m\n\u001b[1;32m   1234\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mevt_handled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m                     self.dialect.do_execute(\n\u001b[0;32m-> 1236\u001b[0;31m                         \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1237\u001b[0m                     )\n\u001b[1;32m   1238\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learn-env/lib/python3.6/site-packages/sqlalchemy/engine/default.py\u001b[0m in \u001b[0;36mdo_execute\u001b[0;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdo_execute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m         \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdo_execute_no_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperationalError\u001b[0m: (sqlite3.OperationalError) near \"Order\": syntax error [SQL: 'SELECT * FROM Order'] (Background on this error at: http://sqlalche.me/e/e3q8)"
     ]
    }
   ],
   "source": [
    "df_ord = pd.read_sql_query(\"SELECT * FROM Order\",  engine)\n",
    "df_ord.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "EZeLgicvwwP6",
    "hLrUyfOEwwQG",
    "O0xNoo6vwwQV",
    "GtI-okJkwwQa",
    "Qk9N1jMywwQf",
    "GDzeDK5gwwQk",
    "ekTOehxHwwQu",
    "XrvSuzkKwwQz",
    "PKDwFRjcwwSn",
    "7hWwTGx1wwTG",
    "5TVC2x_RwwTN"
   ],
   "name": "Copy of Sharing version - JMI - of mod2_project_WIP.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "643px",
    "left": "31px",
    "top": "54px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "85.7969px",
    "left": "975px",
    "right": "20px",
    "top": "-18px",
    "width": "336.391px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
